{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e3fa0e-4706-4ad1-b00a-9ec65c16bdbb",
   "metadata": {},
   "source": [
    "# AstaGuru Job Application Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca26066-ae42-4c87-9a81-0de46c500861",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf526f-81f6-4259-8b20-e512028c011c",
   "metadata": {},
   "source": [
    "### Q1: How will you approach web scraping for auction data, considering potential ethical and legal considerations around copyright and data privacy? What strategies would you use to ensure responsible data collection?  Demonstrate through a working example  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c959a8c-6481-4229-8753-147256bce952",
   "metadata": {},
   "source": [
    "#### Ethical and Legal Considerations\n",
    "*Compliance with Terms of Service* : I would have to review the Terms of Service of the intended website and ensure scraping activities do not violate them. Respecting their rules is essential to avoid legal issues. \n",
    "\n",
    "*Copyright and Data Privacy* : I would ensure that the data collected is used in a manner that respects copyright laws and data privacy regulations (e.g., GDPR in Europe, Personal Data Protection Bill in India). I would avoid collecting personal information unless absolutely necessary, and if needed the personal information can also be made anonymous. \n",
    "\n",
    "*Rate Limiting* : I would implement rate limiting practices to avoid overwhelming the server, ensuring the website remains accessible to other users. This would be done by adding delays between requests to avoid making too many requests in a short period.\n",
    "\n",
    "*Respectful Access* : I would use ethical scraping practices, such as setting a user agent string that identifies the scraper and including contact information in case the website administrators need to reach out. \n",
    "\n",
    "#### Strategies for Responsible Data Collection\n",
    "*Robots.txt* : I would always check the robots.txt file of the website to understand which parts of the site can be legally scraped, and if it can be scraped at all. \n",
    "\n",
    "*API Access* : If the website provides an API, I would prefer using it over scraping HTML pages. APIs are designed for data access and are often more stable and reliable than webscraping. \n",
    "\n",
    "*Data Storage* : I would store data securely and ensure it is accessible only to authorized personnel. Encrypt sensitive data both in transit and at rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998c245-d1d1-42a4-a403-606acaef99c9",
   "metadata": {},
   "source": [
    "#### Example: I want to try and retrieve data about the most expensive paintings ever sold from Wikipedia. This table includes  the name, the image of the painting, the artist, the year of creation and year of sale etc. Here is the link to the page I want to scrape: https://en.wikipedia.org/wiki/List_of_most_expensive_paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c287cbc9-fe5f-4bb2-acec-5a0d0572d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to most_expensive_paintings_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most_expensive_paintings'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Checking to see if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Finding the required table\n",
    "    table = soup.find('table', class_='wikitable')\n",
    "\n",
    "    paintings = []\n",
    "    artists = []\n",
    "    sale_prices = []\n",
    "    sale_dates = []\n",
    "\n",
    "    # Extracting the Data\n",
    "    rows = table.find_all('tr')[1:]  \n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        paintings.append(columns[1].text.strip())\n",
    "        artists.append(columns[3].text.strip())\n",
    "        sale_price = columns[0].text.strip().replace('~', '').replace('[note 3]', '').replace('[25]', '').replace('$', '').strip()\n",
    "        sale_prices.append(sale_price)\n",
    "        sale_dates.append(columns[5].text.strip())\n",
    "\n",
    "    # Creating a DataFrame with the table\n",
    "    data = pd.DataFrame({\n",
    "        'Painting': paintings,\n",
    "        'Artist': artists,\n",
    "        'Sale Price': sale_prices,\n",
    "        'Sale Date': sale_dates\n",
    "    })\n",
    "\n",
    "    # Saving Data to CSV File\n",
    "    data.to_csv('most_expensive_paintings_cleaned.csv', index=False)\n",
    "\n",
    "    print(\"Data scraped and saved to most_expensive_paintings_cleaned.csv\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "203997a9-f387-435a-bfc3-d301e7f99699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Painting</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Sale Price</th>\n",
       "      <th>Sale Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salvator Mundi</td>\n",
       "      <td>Leonardo da Vinci</td>\n",
       "      <td>450.3</td>\n",
       "      <td>November 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interchange</td>\n",
       "      <td>Willem de Kooning</td>\n",
       "      <td>300</td>\n",
       "      <td>September 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Card Players</td>\n",
       "      <td>Paul Cézanne</td>\n",
       "      <td>250 +</td>\n",
       "      <td>April 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nafea Faa Ipoipo(When Will You Marry?)</td>\n",
       "      <td>Paul Gauguin</td>\n",
       "      <td>210</td>\n",
       "      <td>September 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number 17A</td>\n",
       "      <td>Jackson Pollock</td>\n",
       "      <td>200</td>\n",
       "      <td>September 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Painting             Artist Sale Price  \\\n",
       "0                          Salvator Mundi  Leonardo da Vinci      450.3   \n",
       "1                             Interchange  Willem de Kooning        300   \n",
       "2                        The Card Players       Paul Cézanne      250 +   \n",
       "3  Nafea Faa Ipoipo(When Will You Marry?)       Paul Gauguin        210   \n",
       "4                              Number 17A    Jackson Pollock        200   \n",
       "\n",
       "           Sale Date  \n",
       "0  November 15, 2017  \n",
       "1     September 2015  \n",
       "2         April 2011  \n",
       "3     September 2014  \n",
       "4     September 2015  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fabb6b-13c7-4440-84df-e3ecf2fd5b99",
   "metadata": {},
   "source": [
    "### Q2:  How would you identify trends in buyer behavior, item valuation, and market demand for an auction house?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780adfa-65e9-4b46-a770-144a0b3e4577",
   "metadata": {},
   "source": [
    "#### To identify trends, we need to analyze historical data from past auctions. This includes information on items sold, their final prices, buyer demographics, and more. Here are the steps and methodologies to achieve this:\n",
    "\n",
    "#### 1. Data Collection\n",
    "*Historical Auction Data* : I would collect data on past auctions, including item descriptions, final bid prices, auction dates, and buyer details.\n",
    "\n",
    "*External Data* : I would gather additional market data, such as economic indicators, art market reports, and competitor analysis.\n",
    "\n",
    "#### 2. Data Processing and Cleaning\n",
    "*Data Cleaning* : This would involve standard data cleaning processes, such as identifying and removing null values, or removing duplicates. \n",
    "\n",
    "*Data Enrichment* : Additional data points could be included in the database, such as inflation-adjusted prices or conversion rates for international buyers.\n",
    "\n",
    "#### 3. Data Analysis Techniques\n",
    "*Descriptive Statistics* : I would calculate mean, median, and mode of auction prices to understand central tendencies. These are the most basic points of data analysis, but still incredibly important.\n",
    "\n",
    "*Time Series Analysis* : Time series analysis can be leveraged to identify seasonal trends and cyclical patterns in auction prices.\n",
    "\n",
    "*Regression Analysis* : I would employ regression models to determine factors influencing item valuation, such as artist popularity, condition of the item, provenance, etc. I would use correlation calculations to find any relationship between 2 such factors wherein investing 'x' amount in one area will cause a 'greater than x' increase in output/sales. \n",
    "\n",
    "*Cluster Analysis*: Segment buyers into clusters based on purchase behavior and demographics to identify distinct buyer personalities. This could be based on amount spent, the type of art chosen, the number of bids etc. - all these questions could be answered and explored using data analysis techniques. \n",
    "\n",
    "#### 4. Visualization\n",
    "*Trend Graphs* : I would plot historical auction prices to visualize price trends over time.\n",
    "\n",
    "*Heatmaps* : Heatmaps could be used to show demand for different categories of items, or to show the distribution of users in a certain country or the world.\n",
    "\n",
    "*Buyer Segmentation* : Visualizations could be created to depict buyer clusters and their characteristics. This would allow us to customize our marketing procedures based on the characteristics of each buyer cluster\n",
    "\n",
    "#### There are multiple ways to clean, analyze, and visualize historical data to understand trends, behaviors, market prices and demand within the industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3979e4-8946-4b69-b12a-241d85599e8e",
   "metadata": {},
   "source": [
    "### THANK YOU SO MUCH\n",
    "#### Aayush Damani"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
